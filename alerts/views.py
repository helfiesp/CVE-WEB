from django.shortcuts import render
from django.http import HttpResponse, HttpRequest, HttpResponseRedirect
from django.shortcuts import redirect
from django.contrib.auth.decorators import login_required, user_passes_test
from django.core.files.storage import FileSystemStorage
from django.urls import reverse
import os
from django.contrib.auth.models import Group
from datetime import datetime
import scripts.get_cves as get_cves
import scripts.shodan_filter as shodan_filter
from .models import CVEScans,ShodanResults,UnlistedCVEs,QualysResults,PasswordSpray,OKDomains,QualysComments,NessusData,TelegramData, NessusDataIDs
from .forms import OKDomainsForm
import scripts.send_alert_email as alert_email
from collections import defaultdict
import scripts.shodan_ as shodan
import scripts.soc_scripts as soc_scripts
from django.shortcuts import render
from django.conf import settings
from django.core.files.storage import FileSystemStorage
from django.db.models import Q
import requests
import subprocess
import json
import logging
import time
from django.db.models import F
from googletrans import Translator
import csv
import sys
import ast 
import re
try:
    import ldap
except:
    print("Could not import LDAP")


def LoadProxy():
    # Enables the proxy into the environment to allow communication outside of the network.
    os.environ["http_proxy"] = "http://{}:{}@inetproxy.oslofelles.oslo.kommune.no:3128".format(os.environ["CSIRT_LDAPUSER"], os.environ["CSIRT_LDAPPASSWORD"])
    os.environ["https_proxy"] = "http://{}:{}@inetproxy.oslofelles.oslo.kommune.no:3128".format(os.environ["CSIRT_LDAPUSER"], os.environ["CSIRT_LDAPPASSWORD"])

def DisableProxy():
    # Disables the proxy to allow communication with internal sources.
    os.environ.pop("http_proxy", None)
    os.environ.pop("https_proxy", None)

def is_member_of_group(group_name):
    # Checks if ta user is a part of a group.
    def check(user):
        return user.is_authenticated and user.groups.filter(name=group_name).exists()
    return check

@login_required
def GetUserData(request):
    # Fetches the user data stored in the Sqlite3 database.
    user = request.user
    user_data = {
        'username': user.username,
        'email': user.email,
        'first_name': user.first_name,
        'last_name': user.last_name,
    }
    return user_data

@login_required
def index(request):
    return render_cve_page(request, 'Siste døgn', get_cves.daily_cve, additional_context={
        'TelegramData': GetLatestTelegramData(),
        'UserData': GetUserData(request)
    })

@login_required
def cve_weekly(request):
    return render_cve_page(request, 'Siste 7 dager', get_cves.weekly_cve)

@login_required
def cve_monthly(request):
    return render_cve_page(request, 'Denne måneden', get_cves.monthly_cve)

@login_required
def cve_all(request):
    # The 'CVEQuery' value seems to be mistakenly repeated from cve_monthly. 
    # Assuming it should be something like 'All CVEs'. Adjust as needed.
    return render_cve_page(request, 'All CVEs', get_cves.all_cve)

def render_cve_page(request, cve_query, cve_function, additional_context=None):
    """
    Helper function to fetch CVEs, related news, and unlisted CVEs. 
    Builds the context and renders the 'index.html' page.

    Parameters:
        request: The HTTP request object.
        cve_query (str): The query description (e.g., 'Siste døgn').
        cve_function (function): Function to call to retrieve CVEs.
        additional_context (dict, optional): Additional context to be added, if any.

    Returns:
        Rendered HTML page.
    """
    # Fetch data
    found_cves = cve_function()
    found_news = get_cves.daily_news()
    UnlistedCVEs = get_cves.unlisted_cves()
    cve_stats = GetCVEStatistics(found_cves)

    # Build context
    context = {
        'CVE_list': found_cves,
        'CVECount': cve_stats[0],
        'CVEStats': cve_stats[1],
        'CVEQuery': cve_query,
        'DailyNews': found_news,
        'UnlistedCVEs': UnlistedCVEs
    }
    
    # If there's additional context, merge it
    if additional_context:
        context.update(additional_context)

    # Render the page
    return render(request, 'index.html', context)


def GetCVEStatistics(cve_list):
    """
    Generate statistics based on the severity of CVEs.
    """
    # Initialize severity counts
    severity = {'Kritisk':0,'Høy':0,'Medium':0,'Lav':0,'N/A':0}

    # Classify CVEs based on 'cvss_score'
    for cve in cve_list:
        score = cve["cvss_score"]

        if score == "N/A":
            severity["N/A"] += 1
        elif score >= 9:
            severity["Kritisk"] += 1
        elif score >= 7.5:
            severity["Høy"] += 1
        elif score >= 5:
            severity["Medium"] += 1
        else:
            severity["Lav"] += 1

    return len(cve_list), severity


# Deprecated code, not currently in use
@login_required
def send_alert(request):
    if request.method == "POST":
        selected_cves = request.POST.getlist("send_alert")
        if len(selected_cves) == 0:
            context = {'AlertMessage':'Du må velge CVEer fra CVE listen for å kunne sende en varsling.'}
        else:
            context = {'Selected_CVEs':get_cves.single_cve(selected_cves)}
    return render(request,'send_alert.html', context)

# Deprecated code, not currently in use
@login_required
def send_alert_email(request):
    if request.method == "POST":
        table = request.POST.get("CVE_Table")
        receiver = request.POST.get("alert_receiver")
        cc = request.POST.get("alert_cc")
        alert_text = request.POST.get("alert_text")
        CVE = request.POST.getlist("CVEs")
        context = {'receiver':receiver, 'cc':cc, 'description':alert_text, 'CVES':CVE}
        alert_email.main(context)
        found_cves = get_cves.daily_cve()
        scan_info = str(*CVEScans.objects.filter(scan_type="daily").values('scan_end').last().values())
        context = {'message':'<b>Sucess</b>: E-mail sent to: {}'.format(receiver),'CVE_list':found_cves, 'CVE_Count':len(found_cves), 'Query':'Realtime CVE', 'Last_scan':scan_info}
        return render(request,'index.html', context)


@login_required
def Shodan(request):
    context = {'UserData':GetUserData(request)}
    return render(request,'shodan.html', context)

@login_required
def update_shodan_entry(request):
    if request.method == "POST":
        comments = request.POST.getlist("comment")
        ids = request.POST.getlist("entry_id")
        for x,y in zip(ids, comments):
            entry = ShodanResults.objects.get(id=x)
            entry.entry_comments = y
            entry.save(update_fields=['entry_comments'])
    filters = ShodanGetFilters(request)
    shodan_results = ShodanSearchQuery(filters)
    context = BuildShodanContext(filters, shodan_results, request)
    return render(request, 'shodan.html', context)


@login_required
def ShodanSearch(request):
    if request.method == "POST":
        filters = ShodanGetFilters(request)
        shodan_results = ShodanSearchQuery(filters)
        context = BuildShodanContext(filters, shodan_results, request)
        return render(request, 'shodan.html', context)
    else:
        shodan_info = shodan.GetResults(0)
        context = {
            'ShodanResults': shodan_info[0],
            'Shodan_Stats': shodan_info[1],
            'ShodanScans': shodan_info[2],
            'Vulns': shodan_info[3],
            'filters': None,
            'UserData': GetUserData(request)
        }
        return render(request, 'shodan.html', context)


def BuildShodanContext(filters, shodan_results, request):
    shodan_info = shodan.GetResults(0)
    context = {
        'ShodanResults': shodan_results,
        'Shodan_Stats': shodan_info[1],
        'ShodanScans': shodan_info[2],
        'filters': filters,
        'UserData': GetUserData(request)
    }
    return context

def ShodanSearchQuery(filters):
    all_data = ShodanResults.objects.values()  # Starting with all results

    # Filter based on the provided criteria
    filtered_data = []
    for entry in all_data:
        scan_data = eval(entry["scan_data"])  # Deserialize the scan_data
        is_webserver = False
        is_ssh = False

        # Determine if entry is a web server
        webservers = ["apache", "lighttpd", "httpd", "microsoft iis", "nginix"]
        for x in webservers:
            if x in str(scan_data.get("product", "")).lower():
                is_webserver = True
                break
        if str(scan_data.get("port", "")) == "80" or str(scan_data.get("port", "")) == "443":
            is_webserver = True

        # Determine if entry is SSH
        if scan_data.get("port", "") == 22 or "ssh" in str(scan_data.get("product", "")).lower():
            is_ssh = True

        # Determine the inclusion based on filters
        check = False
        if filters["webservers"] and is_webserver:
            if not filters["others"]:
                check = True
        if filters["ssh"] and is_ssh:
            if not filters["others"]:
                check = True
        if filters["others"] and not (is_webserver or is_ssh):
            check = True

        # Additional checks for search term/type filters
        if filters["search_type"] == "hostname" and filters["search_term"] not in scan_data.get("hostnames", []):
            continue
        elif filters["search_type"] == "ip" and filters["search_term"] != scan_data.get("ip_str"):
            continue
        elif filters["search_type"] == "product" and filters["search_term"] not in scan_data.get("product", ""):
            continue
        elif filters["search_type"] == "port" and filters["search_term"] not in str(scan_data.get("port", "")):
            continue

        if check:
            if entry not in filtered_data:
                
                filtered_data.append(entry)

        if not filters["search_term"]:
            if not filters["ssh"] and not filters["webservers"]:
                if not filters["others"]:
                    if entry not in filtered_data:
                        filtered_data.append(entry)

    for entry in filtered_data:
        if isinstance(entry["scan_data"], str):
            entry["scan_data"] = eval(entry["scan_data"])

    return filtered_data


def ShodanGetFilters(request):
    filters = {
        "webservers": request.POST.get("webserver_checkbox") == "1", 
        "ssh": request.POST.get("ssh_checkbox") == "1",
        "others": request.POST.get("others_checkbox") == "1",
        "search_type": str(request.POST.get("search_type", None)),
        "search_term": str(request.POST.get("search_term", None)),
    }
    return filters

def ShodanDataFilter(shodan_data, filters):
    webserver_shodan_data = []
    ssh_shodan_data = []
    others_shodan_data = []

    for entry in shodan_data:
        if "ssh" in entry["entry_data"]["product"].lower() or entry["entry_data"]["port"] == 22:
            ssh_shodan_data.append(entry)
        else:
            target_words = ["apache", "http", "iis", "jetty", "nginx"]
            product_data = entry["entry_data"]["product"]
            entry["has_target_word"] = any(word in product_data.lower() for word in target_words)
            status_code = entry["entry_data"]["http"]["status"] if "http" in entry["entry_data"] else None
            if entry["has_target_word"] or entry["entry_data"]["port"] in [80, 443] or (status_code and 200 <= status_code <= 399):
                if entry not in webserver_shodan_data:
                    webserver_shodan_data.append(entry)
            else:
                others_shodan_data.append(entry)

    if filters["ssh"]:
        if filters["webservers"]:
            return ssh_shodan_data.extend(webserver_shodan_data)
        else:
            return ssh_shodan_data
    else:
        if filters["webservers"]:
            return webserver_shodan_data
        else:
            return others_shodan_data



@login_required
def SOC(request):
    context = {'':''}
    return render(request,'soc.html', context)

@login_required
def SOC_Scan_Url(request):
    if request.method == "POST":
        url = request.POST.get("scan_url_entry")
        context = {'URLScan':soc_scripts.URLScanIO(url)}
        return render(request,'soc.html', context)

def GetQualysStats(qualys_results, current_query):
    severity = {'5':0,'4':0,'3':0,'2':0,'1':0}
    internet_exposed = 0
    internet_exposed_ips = []
    for entry in qualys_results:
        severity["{}".format(entry["severity"])] += 1
        if entry["internet_exposed"] == "1":
            internet_exposed += 1
            if entry["hostname"] not in internet_exposed_ips:
                internet_exposed_ips.append(entry["hostname"])
    qualys_stats = {'Timestamp':GetQualysLastEntry(),'Current query':current_query, 'Amount of entries':len(qualys_results), 'Severity':severity, 'Entries with internet exposed':internet_exposed,'Unique internet exposed assets': len(internet_exposed_ips)}
    return qualys_stats


def GetQualysLastEntry():
    # Fetches the latest entry from thr qualys database.
    # This is to ensure that the newest data is shown.
    try:
        last_entry = QualysResults.objects.values().last()["scan_time"]
        return last_entry
    except:
        last_entry = "N/A"

def GetCisaVulns():
    # Fetches the CISA known exploited vulnerabilities from the CISA website.
    cve_list = []
    url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
    try:
        LoadProxy()
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if one occurred.
        cisa_data = response.json()
        
        for entry in cisa_data["vulnerabilities"]:
            if entry["cveID"] not in cve_list:
                cve_list.append(entry["cveID"])
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from {url}: {e}")
        return []
    finally:
        DisableProxy()
    return cve_list

def UpdateQualysCISAField(entry_id, entry_value):
    # Updates the known_exploited field in the QualysDB
    entry = QualysResults.objects.get(id=entry_id)
    entry.known_exploited = entry_value
    entry.save()

def GetQualysBS():
    business_services = ['All data'] + list(QualysResults.objects.exclude(bs__isnull=True).exclude(bs__exact='').values_list('bs', flat=True).distinct())
    sorted_business_services = sorted(business_services)
    return sorted_business_services

@login_required
def Qualys(request):
    last_entry = GetQualysLastEntry()
    context = {'filters':{'dataset':GroupChecker(request), 'last_entry':last_entry}, 'UserData':GetUserData(request), 'business_service':GetQualysBS()}
    return render(request,'qualys.html', context)

def KartoteketAPIBSCheck(request):

    # Checks the Kartoteket API to see if the user is manager of a business service.
    # This is to ensure that the user can automatically view the data assigned to them.
    email = request.user.email
    url = "https://kartoteket.oslo.kommune.no//get-api/tilganger/?email={}".format(email)
    header = {"key": os.environ["KARTOTEKET_NETWORK_API_SECRET_BS"]}
    result = requests.get(url, headers=header).json()["business_services"]
    return result

def ManualCheck(request):
    groups = {
        "CSIRT": ["all"],
        "ITAS": ["OK-ITAS"],
        "ACOS_UKE": ["OK-Acos - Standard tjeneste"]
    }
    user_groups = []
    for group in groups:
        if is_member_of_group(group)(request.user):
            user_groups.extend(groups[group])
    return user_groups

def GroupChecker(request):
    try:
        dataset = KartoteketAPIBSCheck(request)
        if dataset:
            return dataset
        else:
            dataset = ManualCheck(request)
    except Exception as e:
        dataset = ManualCheck(request)
    return dataset


def QualysSearchQuery(filters):
    # Create a dictionary to map filter types to fields
    filter_mapping = {
        "hostname": "hostname",
        "bss": "bss",
        "bs": "bs",
        "system": "system",
        "cve": "cve",
        "vulnerability_name": "title",
        "filepath": "filepath",
        "internal_ip": "ip",
        "external_ip": "internet_exposed",
        "comments": "comments",
        "qualys_id": "qualys_id"
    }
    
    # Construct the base query set
    base_query = QualysResults.objects.filter(scan_time=GetQualysLastEntry())
    
    # Apply filters
    if filters["dataset"] != ["all"]:
        base_query = base_query.filter(bs__in=filters["dataset"])
    if filters["internet_exposed"]:
        base_query = base_query.exclude(internet_exposed__contains="e")
    if filters["critical_vulns"]:
        base_query = base_query.filter(severity="5")
    if filters["known_exploited"]:
        base_query = base_query.filter(known_exploited="1")
    
    # Add the search field filter
    search_field = filter_mapping[filters["search_type"]]
    search_value = filters["vuln_search"]
    search_query = {f"{search_field}__contains": search_value}
    base_query = base_query.filter(**search_query)
    
    # Remove entries containing "Update for kernel" if filter_kernel is true
    if filters["filter_kernel"]:
        base_query = base_query.exclude(title__contains="Update for kernel")
    if filters["filter_false"]:
        base_query = base_query.exclude(false_positive=1)   
    if filters["filter_2s_responsible"]:
        base_query = base_query.exclude(title__contains="CentOs Security Update").exclude(title__contains="Red Hat Update").exclude(title__contains="OpenSSH Command Injection Vulnerability (Generic)").exclude(hostname__contains="ora00")

    # Check and apply selected_service filter
    selected_service = filters.get("selected_service")
    if selected_service and selected_service != "All data":
        base_query = base_query.filter(bs__contains=selected_service)
    
    # Get the query results
    qualys_server_results = base_query.values()
    return QualysDataFilter(qualys_server_results)


def QualysOverview(qualys_data, filters):
    grouped_data = {}
    total_count = 0

    for entry in qualys_data:
        title = entry['title']
        if title in grouped_data:
            grouped_data[title]['count'] += 1
        else:
            grouped_data[title] = {'title': title, 'count': 1}

        if 'severity' not in grouped_data[title]:
            grouped_data[title]['severity'] = entry['severity']

        if 'cve' not in grouped_data[title]:
            grouped_data[title]['cve'] = entry['cve']

        if 'hostnames' not in grouped_data[title]:
            grouped_data[title]['hostnames'] = []
        grouped_data[title]['hostnames'].append(entry['hostname'])

        # Add qualys_id field to the grouped data
        if 'qualys_id' in entry:
            if 'qualys_id' not in grouped_data[title]:
                grouped_data[title]['qualys_id'] = entry['qualys_id']

        # Fetch comments from QualysComments table based on title
        comments = QualysComments.objects.filter(title=title, filters=MapQualysCommentsFilters(filters))
        comment_list = [comment.comment for comment in comments]
        grouped_data[title]['comments'] = ", ".join(comment_list)

        total_count += 1  # Increment the total count

    result = sorted(grouped_data.values(), key=lambda x: x['count'], reverse=True)
    return {'data': result, 'total_count': total_count}



def update_comments_for_qualys_results(entries):
    for entry in entries:
        title = entry.get("title")
        comment = entry.get("comment")
        if comment and len(comment) > 5:
            entries_to_update = QualysResults.objects.filter(title=title)
            for item in entries_to_update:
                if item.comments != comment:
                    item.comments = comment
                    item.save()

def update_comments_for_qualys_comments(filters, servers, entries):
    mapped_filters = MapQualysCommentsFilters(filters)
    for entry in entries:
        title = entry.get("title")
        comment = entry.get("comment")
        if comment and len(comment) > 5:
            obj, created = QualysComments.objects.get_or_create(
                title=title, filters=mapped_filters, servers=str(servers))
            if obj.comment != comment:
                obj.comment = comment
                obj.save()

def UpdateQualysComments(request):
    if request.method == "POST":

        # Extract filters from the request
        filters = dict(eval(request.POST.get("filters")))
        print(filters)
        # Filter out the keys based on their prefixes
        comment_keys = [k for k in request.POST.keys() if k.startswith("comment_")]
        team_keys = [k for k in request.POST.keys() if k.startswith("team_")]
        false_positive_keys = [k for k in request.POST.keys() if k.startswith("false_positive_checkbox-")]

        # Set all false_positives to 0
        all_entry_ids = [int(key.split("_")[1]) for key in comment_keys]
        QualysResults.objects.filter(id__in=all_entry_ids).update(false_positive=0)

        # Fetch values based on the filtered keys
        comments = [request.POST[k] for k in comment_keys]
        teams = [request.POST[k] for k in team_keys]
        false_positives = [request.POST[k] for k in false_positive_keys]

        for key, comment in zip(comment_keys, comments):
            entry_id = int(key.split("_")[1])  # Assuming the key is in the format "comment_ID"
            
            # Only update comment if it's not None or not an empty string
            if comment and comment.strip():
                try:
                    QualysResults.objects.filter(id=entry_id).update(comments=comment)
                except Exception as e:
                    # Handle exceptions (like not finding the entry) here if needed
                    print(f"Error updating comment for entry ID {entry_id}: {e}")

        for key, team in zip(team_keys, teams):
            entry_id = int(key.split("_")[1])  # Assuming the key is in the format "team_ID"
            
            # Only update team if it's not None or not an empty string
            if team and team.strip():
                try:
                    QualysResults.objects.filter(id=entry_id).update(team=team)
                except Exception as e:
                    # Handle exceptions here if needed
                    print(f"Error updating team for entry ID {entry_id}: {e}")

        for key, false_positive in zip(false_positive_keys, false_positives):
            try:
                entry_id = int(key.split("-")[1])  # Attempt to extract the ID
            except ValueError:
                # Couldn't convert to int, so skip this key
                continue
    
            false_positive_value = 1 if false_positive == "1" else 0  # Convert the checkbox value to 1 or 0

            try:
                QualysResults.objects.filter(id=entry_id).update(false_positive=false_positive_value)
            except Exception as e:
                # Handle exceptions here if needed
                print(f"Error updating false_positive for entry ID {entry_id}: {e}")

        # Call the QualysSearch function with the extracted filters
        return QualysSearch(request, filters)





def MapQualysCommentsFilters(filters):
    new_filters = {
    'dataset':filters["selected_service"], 
    'internet_exposed':filters["internet_exposed"],
    'critical_vulns':filters["critical_vulns"], 
    'known_exploited':filters["known_exploited"]
    }
    return new_filters


def QualysDataFilter(qualys_data):
    checklist = []
    last_detected = GetQualysLastEntry()  # Assuming GetQualysLastEntry() returns the desired value 
    for entry in qualys_data:
        if entry["id"] not in checklist:
            checklist.append([entry["id"], entry]) 
    qualys_data_filtered = []
    for entry in checklist:
        if entry[1]["systemmanager"]:
            entry[1]["systemmanager"] = entry[1]["systemmanager"].translate(str.maketrans('', '', "[]'"))
        if "e" in entry[1]["internet_exposed"]:
            entry[1]["internet_exposed"] = "N/A"      
        if entry[1]["last_fixed"] == "NaT":
            entry[1]["last_fixed"] = "N/A"        
        if entry[1]["cve"] == "nan":
            entry[1]["cve"] = "N/A"
        #if entry[1]["last_detected"].split()[0] != last_detected:
            #continue  # Skip this entry if last_detected is not equal to the desired value
        qualys_data_filtered.append(entry[1])
    return qualys_data_filtered

def QualysGetStats(qualys_results):
    pass

def QualysGetFilters(request, dataset):
    last_entry = GetQualysLastEntry()
    filters = {
        "last_entry":last_entry,
        "dataset": dataset,
        "filepath": request.POST.get("filepath_checkbox"), 
        "internet_exposed":request.POST.get("internetex_checkbox"),
        "overview":request.POST.get("overview_checkbox"),
        "critical_vulns": request.POST.get("critical_vulns_checkbox"), 
        "all_entries":request.POST.get("all_checkbox"), 
        "known_exploited":request.POST.get("knownex_checkbox"),
        "vuln_search":request.POST.get("vuln_search"),
        "search_type":request.POST.get("search_type"),
        "selected_service":request.POST.get("selected_service"),
        "systeminfo":request.POST.get("systeminfo_checkbox"),
        "business_service":request.POST.get("business_service_checkbox"),
        "filter_kernel":request.POST.get("filter_kernel_checkbox"),
        "filter_false":request.POST.get("filter_false_checkbox"),
        "filter_2s_responsible":request.POST.get("filter_2s_responsible_checkbox"),
        "selected_team":request.POST.get("team_picker"),
        }
    return filters

@login_required
def QualysSearch(request, filters=None, vulnerability=None, hostname=None):
    if request.method == "POST":
        business_services = GetQualysBS()
        if not filters:
            filters = QualysGetFilters(request, GroupChecker(request))
        qualys_results = QualysSearchQuery(filters)
        context = build_context(filters, qualys_results, request, business_services)
        return render(request,'qualys.html', context)
    if vulnerability or hostname:
        business_services = GetQualysBS()
        filters = dict(eval(request.GET.get('filters', None)))
        if vulnerability:
            filters["search_type"] = "qualys_id"
            filters["vuln_search"] = vulnerability
        elif hostname:
            filters["search_type"] = "hostname"
            filters["vuln_search"] = hostname 
        qualys_results = QualysSearchQuery(filters)
        context = build_context(filters, qualys_results, request, business_services)
        return render(request,'qualys.html', context)
    else:
        message = "Du må velge et filter for å vise resultater..." 
        context = {'message': message, 'UserData': GetUserData(request)}
        return render(request, 'qualys.html', context)


def build_context(filters, qualys_results, request, business_services):
    context = {
        'QualysResults': qualys_results,
        'filters': filters,
        'UserData': GetUserData(request),
        'business_service': business_services
    }
    
    if filters.get("overview"):
        context["overview_table"] = QualysOverview(qualys_results, filters)
    
    return context


@login_required
def ExposedPasswords(request):
    data = PasswordSpray.objects.values()
    for entry in data:
        for item in list(eval(entry["userinfo"])):
            if entry["user"] in str(item):
                for x in item:
                    if type(x) == dict:
                        entry["groups"] = x["memberOf"]
    context = {'data':data, 'UserData':GetUserData(request)}
    return render(request,'exposed_passwords.html', context)

@login_required
def VulnerabilityUpload(request):
    if request.method == 'POST':
        file = request.FILES['file']
        filename = file.name
        upload_path = '/var/csirt/source/CVE-WEB/uploads/qualys_vulns/'

        # Get the selected vulnerability type from the dropdown menu
        vulnerability_type = request.POST.get('vulnerability_type')

        # Check if the selected vulnerability type is Qualys or Defender and set the filename accordingly
        if vulnerability_type == 'Qualys':
            new_filename = 'QualysVulnerabilities.xlsx'
            script_path = '/var/csirt/source/CVE-WEB/scripts/updatequalys.sh'
        elif vulnerability_type == 'Defender':
            new_filename = 'DefenderVulnerabilities.xlsx'
            script_path = '/var/csirt/source/CVE-WEB/scripts/updatedefender.sh'
        else:
            # Return an error message if the selected vulnerability type is invalid
            context = {'filename': '', 'upload_result': 'error', 'output': '', 'error': 'Invalid request'}
            return render(request, 'qualys_upload.html', context)

        file_path = os.path.join(upload_path, new_filename)

        # Remove the existing "Vulnerabilities.xlsx" file if it exists
        existing_file_path = os.path.join(upload_path, new_filename)
        if os.path.isfile(existing_file_path):
            os.remove(existing_file_path)

        # Save the uploaded file with the appropriate filename
        with open(file_path, 'wb') as f:
            for chunk in file.chunks():
                f.write(chunk)

        # Run the appropriate script based on the selected vulnerability type
        process = subprocess.Popen(['/bin/bash', script_path, file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, error = process.communicate()

        # Decode the output and error from bytes to string
        output = output.decode()
        error = error.decode()

        # Render the qualys_upload.html template with the upload result and script output
        context = {'filename': new_filename, 'upload_result': 'success', 'output': output, 'error': error, 'UserData':GetUserData(request)}
        return render(request, 'qualys_upload.html', context)

    return render(request, 'qualys_upload.html')

def GetLatestNessusData():
    latest_nessus_data = NessusData.objects.order_by('-date').first()
    if latest_nessus_data:
        latest_data = json.loads(latest_nessus_data.data)
        dataset = latest_data.get("data")
        return dataset

@login_required
def AttackSurface(request):
    vulnerabilities = process_vulnerabilities(GetLatestNessusData())
    context = {'Vulnerabilities': vulnerabilities, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface.html', context)

def AttackSurfaceVulnStatistics(vulnerabilities):
    statistics = {
        'total_vulnerabilities': len(vulnerabilities),
        'total_CVE': 0,
        'average_CVSS': 0,
        'risk_count': {
            'Critical': 0,
            'High': 0,
            'Medium': 0,
            'Low': 0,
            'None': 0,

        },
    }

    total_cvss_score = 0

    for vuln in vulnerabilities:
        vuln = vuln["vuln"]
        # Counting CVEs
        if vuln.get("CVE"):
            statistics['total_CVE'] += 1

        # Adding up CVSS scores
        cvss = vuln.get("CVSS")
        if cvss is not None:
            total_cvss_score += cvss

        # Counting Risk levels
        risk = vuln.get("Risk", "Other")
        statistics['risk_count'][risk] = statistics['risk_count'].get(risk, 0) + 1

    # Calculate average CVSS score
    if len(vulnerabilities) > 0:
        statistics['average_CVSS'] = total_cvss_score / len(vulnerabilities)

    statistics["risk_count"]["Low"] += statistics["risk_count"]["None"]
    return statistics

@login_required
def AttackSurfaceSearchDomain(request, domain):
    if request.method == 'POST':
        domain_comment = request.POST.get('comment_{}'.format(domain))
        ok_domain = OKDomains.objects.filter(domain=domain).first()
        if ok_domain:
            ok_domain.comments = domain_comment
            ok_domain.save()

            # Update comments and false positives for individual vulnerabilities
            for vuln in process_vulnerabilities(json.loads(ok_domain["vulnerabilities"])):
                plugin_id = vuln['vuln']['Plugin_ID']
                vuln_comment = request.POST.get('vuln_comment_{}'.format(plugin_id))
                vuln_false_positive = request.POST.get('false_positive_checkbox_{}'.format(plugin_id)) == 'on'
                nessus_data = NessusDataIDs.objects.filter(domain=domain, plugin_id=plugin_id).first()
                if nessus_data:
                    nessus_data.comments = vuln_comment
                    nessus_data.false_positive = vuln_false_positive
                    nessus_data.save()


    domain_entry = OKDomains.objects.filter(domain=domain).values()[0]
    vulnerabilities = process_vulnerabilities(json.loads(domain_entry["vulnerabilities"]))
    
    try:
        # Extract comments and false positives for this domain
        comments_dict = dict(item.split(":") for item in domain_entry["plugin_comments"].split(",") if item)
        fp_dict = dict(item.split(":") for item in domain_entry["vuln_false_positive"].split(",") if item)
    except:
        pass
    
    # Add comments and false positives to each vulnerability
    try:
        for vuln in vulnerabilities:
            vuln['vuln']['Description'] = vuln['vuln']['Description'].replace("Note that Nessus has not tested for these issues but has instead relied only on the application's self-reported versionnumber.", "")
            
            if len(vuln['vuln']['Description']) > 150:
                vuln['vuln']['Description'] = "N/A"

            plugin_id = vuln['vuln']['Plugin_ID']
            if plugin_id == "10335":
                print("HEY")

            vuln['vuln']['comment'] = comments_dict.get(plugin_id, "")
            vuln['vuln']['false_positive'] = fp_dict.get(plugin_id) == "1"

        
    except:
            pass

    filters = {'search_type': 'domene', 'vuln_search': domain}
    context = {
        'okdomains': domain_entry,
        'Domain': domain,
        'Vulnerabilities': vulnerabilities,
        'PortInformation':port_information,
        'UserData': GetUserData(request),
        'filters': filters,
        'statistics': AttackSurfaceVulnStatistics(vulnerabilities)
    }
    return render(request, 'attack_surface_domain_search.html', context)


@login_required
def AttackSurfaceSearchByType(request):
    if request.method == "POST":
        vuln_search = request.POST.get("vuln_search")
        search_type = request.POST.get("search_type")
        filters = {"vuln_search":vuln_search, "search_type":search_type}

        if search_type == "domain":
            # Construct the URL using the provided domain value
            redirect_url = reverse('attack_surface_search_by_domain', kwargs={'domain': vuln_search})
            return HttpResponseRedirect(redirect_url)

        elif search_type in ["cve", "description", "name", "synopsis"]:
            return AttackSurfaceSearch(request, filters)

        else:
            print("Invalid search type.")
    else:
        return render(request, 'attack_surface_search_form.html', {'UserData': GetUserData(request)})


def filter_vulnerabilities(vulnerabilities, key, value):
    filtered_vulnerabilities = []
    for entry in vulnerabilities:
        if str(value).lower() in str(entry["vuln"][key]).lower():
            filtered_vulnerabilities.append(entry)
    return filtered_vulnerabilities

@login_required
def AttackSurfaceSearch(request, filters):
    dataset = GetLatestNessusData()
    vulnerabilities = process_vulnerabilities(dataset)
    
    filtered_vulnerabilities = []

    if filters["search_type"] == "cve":
        filtered_vulnerabilities = [entry for entry in vulnerabilities if str(entry["vuln"]["CVE"]).lower() == str(filters["vuln_search"]).lower()]
    elif filters["search_type"] == "description":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Description", filters["vuln_search"])
    elif filters["search_type"] == "name":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Name", filters["vuln_search"])
    elif filters["search_type"] == "synopsis":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Synopsis", filters["vuln_search"])

    context = {'filters':filters, 'Vulnerabilities': filtered_vulnerabilities, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface.html', context)



def process_vulnerabilities(dataset):
    grouped_entries = defaultdict(lambda: {'vuln': {}, 'affected_hosts': set()})

    blacklist = ['Web Application Cookies Not Marked Secure', "HTTP Cookie 'secure' Property Transport Mismatch",
    'Patch Report', 'Apache HTTP Server Version', 'Missing or Permissive Content-Security-Policy frame-ancestors HTTP Response Header',
    'Missing or Permissive X-Frame-Options HTTP Response Header', 'External URLs','Web Application Cookies Not Marked HttpOnly',
    'HyperText Transfer Protocol (HTTP) Redirect Information','HyperText Transfer Protocol (HTTP) Information',
    'HTTP Methods Allowed (per directory)','HTTP TRACE / TRACK Methods Allowed']

    def IterateEntry(entry):
        entry_name = entry.get("Name")
        if entry_name:
            vulnerability_info = {
                "Plugin_ID": entry.get("Plugin ID"),
                "CVE": entry.get("CVE") if entry.get("CVE") else None,
                "CVSS": float(entry.get("CVSS v2.0 Base Score")) if entry.get("CVSS v2.0 Base Score") else float(0.0),
                "Name": entry_name,
                "Risk": entry.get("Risk"),
                "Synopsis": entry.get("Synopsis"),
                "Description": entry.get("Description"),
                "Solution": entry.get("Solution"),
                "See_Also": entry.get("See Also"),
                "Plugin_Output": entry.get("Plugin Output")
            }

            affected_host = {
                "Host": entry.get("Host"),
                "Protocol": entry.get("Protocol"),
                "Port": entry.get("Port")
            }



            grouped_entries[entry_name]['vuln'] = vulnerability_info
            grouped_entries[entry_name]['affected_hosts'].add(affected_host["Host"])  # Using a set to remove duplicates

    if isinstance(dataset, list):
        for entry in dataset:
            if entry["Name"] not in blacklist:
                IterateEntry(entry)
    else:
        IterateEntry(dataset)

    # Convert the grouped entries dictionary into a list of dictionaries
    grouped_lists = list(grouped_entries.values())
    
    # Sort the grouped lists based on the highest CVSS value within 'vuln'
    grouped_lists.sort(key=lambda x: ('Critical' not in x['vuln']['Risk'], -x['vuln']['CVSS']))
    return grouped_lists


@login_required
def AttackSurfaceSearchPlugin(request, plugin_id):
    if request.method == "GET":
        query = Q(vulnerabilities__contains=f'"Plugin ID": "{plugin_id}"')
        matching_entries = OKDomains.objects.filter(query).values()
        vuln_data = None

        try:
            # Fetch the NessusDataIDS object for the given plugin_id
            nessus_data_id = NessusDataIDs.objects.get(plugin_id=plugin_id)
            # Store the comment in a variable
            fetched_comment = nessus_data_id.comment
        except NessusDataIDs.DoesNotExist:
            fetched_comment = None 

        modified_matching_entries = []
        for item in matching_entries:
            vulnerabilities = json.loads(item["vulnerabilities"])
            plugin_output_modified = None
            for vuln in vulnerabilities:
                if str(vuln.get("Plugin ID")) == str(plugin_id):
                    vuln_data = process_vulnerabilities([vuln])[0]['vuln']
                    plugin_output_modified = vuln_data["Plugin_Output"]
                    break

            item_dict = dict(item)  # Convert the item to a dictionary if it's not already one
            item_dict['Plugin_Output'] = plugin_output_modified  # Add the plugin_output_modified to the dictionary
            modified_matching_entries.append(item_dict)


        context = {'Vulnerabilities': modified_matching_entries, 'Plugin_ID': plugin_id, 'vuln_data': vuln_data, 'UserData': GetUserData(request), 'fetched_comment': fetched_comment}
        return render(request, 'attack_surface_vuln_search.html', context)

    

@login_required
def AttackSurfaceDomains(request):
    okdomains = OKDomains.objects.all()
    
    # Parse the JSON data in the vulnerabilities field for each domain
    for domain in okdomains:
        if domain.vulnerabilities:
            domain.vulnerabilities = list(eval(domain.vulnerabilities))
        else:
            domain.vulnerabilities = []  # Set as empty list if the field is empty

    context = {'okdomains': okdomains, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface_domains.html', context)

def UpdateVulDomainComments(request):    
    if request.method == "POST":
        plugin_id = request.POST.get("plugin_id")
        
        comment_keys = [k for k in request.POST.keys() if k.startswith("comment_")]
        false_positive_keys = [k for k in request.POST.keys() if k.startswith("false_positive_checkbox-")]
        all_domains_for_plugin = OKDomains.objects.filter(plugin_comments__contains=str(plugin_id))
        
        # Set comments for the vulnerability
        vuln_comment = request.POST.get(f"vuln_comment_{plugin_id}")
        nessus_data_id, created = NessusDataIDs.objects.get_or_create(plugin_id=plugin_id)
        nessus_data_id.comment = vuln_comment
        nessus_data_id.save()
        
        # Set comments for the individual domains
        for key in comment_keys:
            domain_name = key.replace("comment_", "")
            new_comment = request.POST[key].strip()

            if new_comment:
                try:
                    domain = OKDomains.objects.get(domain=domain_name)
                    if not domain.plugin_comments:
                        domain.plugin_comments = ""
                    comments_dict = dict(item.split(":") for item in domain.plugin_comments.split(",") if item)
                    comments_dict[plugin_id] = new_comment
                    updated_comments = ",".join(f"{k}:{v}" for k, v in comments_dict.items())
                    domain.plugin_comments = updated_comments
                    domain.save()
                    
                except Exception as e:
                    pass
        
        # Set false positive values for the individual domains
        for domain in all_domains_for_plugin:
            checkbox_name = f"false_positive_checkbox-{plugin_id}-{domain.domain}"
            if checkbox_name in false_positive_keys:
                # Checkbox was checked
                if not domain.vuln_false_positive:
                    domain.vuln_false_positive = ""
                fp_dict = dict(item.split(":") for item in domain.vuln_false_positive.split(",") if item)
                fp_dict[str(plugin_id)] = "1"
                domain.vuln_false_positive = ",".join(f"{k}:{v}" for k, v in fp_dict.items())
            else:
                # Checkbox was unchecked
                if domain.vuln_false_positive:
                    fp_dict = dict(item.split(":") for item in domain.vuln_false_positive.split(",") if item)
                    if str(plugin_id) in fp_dict:
                        del fp_dict[str(plugin_id)]
                    domain.vuln_false_positive = ",".join(f"{k}:{v}" for k, v in fp_dict.items())
    
            domain.save()

        # Redirect back
        redirect_url = reverse('attack_surface_search_by_plugin', kwargs={'plugin_id': int(plugin_id)})
        return HttpResponseRedirect(redirect_url)
    
    else:
        return HttpResponse("Invalid Request Method", status=405)


def UpdateDomainComments(request):    
    if request.method == "POST":
        domain_name = request.POST.get("domain")
        domain_comment = request.POST.get(f"comment_{domain_name}")
        comment_keys = [k for k in request.POST.keys() if k.startswith("vulnerability_comment_")]
        false_positive_keys = [k for k in request.POST.keys() if k.startswith("false_positive_checkbox-")]

        # Update the domain comments field
        OKDomains.objects.filter(domain=domain_name).update(comments=domain_comment)

        # Fetch the domain object
        try:
            domain_obj = OKDomains.objects.get(domain=domain_name)
            
            # Update comments for individual vulnerabilities
            for key in comment_keys:
                plugin_id = key.replace("vulnerability_comment_", "")
                new_comment = request.POST[key].strip()

                if new_comment:
                    if not domain_obj.plugin_comments:
                        domain_obj.plugin_comments = ""
                    comments_dict = dict(item.split(":") for item in domain_obj.plugin_comments.split(",") if item)
                    comments_dict[plugin_id] = new_comment
                    updated_comments = ",".join(f"{k}:{v}" for k, v in comments_dict.items())
                    domain_obj.plugin_comments = updated_comments

            # Update false positive values for individual vulnerabilities
            for key in false_positive_keys:
                _, plugin_id, _ = key.split('-')
                
                if not domain_obj.vuln_false_positive:
                    domain_obj.vuln_false_positive = ""
                fp_dict = dict(item.split(":") for item in domain_obj.vuln_false_positive.split(",") if item)
                
                # Checkbox was checked
                fp_dict[str(plugin_id)] = "1"
                domain_obj.vuln_false_positive = ",".join(f"{k}:{v}" for k, v in fp_dict.items())

            false_positives = []
            for entry in false_positive_keys:
                false_positives.append(entry.split("-")[1])
            try:
                fp_dict = dict(item.split(":") for item in domain_obj.vuln_false_positive.split(",") if item)
                for item in domain_obj.vuln_false_positive.split(","):
                    if item.split(":")[0] not in false_positives:
                        del fp_dict[str(item.split(":")[0])]
                domain_obj.vuln_false_positive = ",".join(f"{k}:{v}" for k, v in fp_dict.items())
            except:
                pass
            # Save the domain object after all updates
            domain_obj.save()

            # Redirect back or to a desired location
            redirect_url = reverse('attack_surface_search_by_domain', kwargs={'domain': domain_name})
            return HttpResponseRedirect(redirect_url)
        
        except OKDomains.DoesNotExist:
            return HttpResponse("Domain not found", status=404)
    else:
        return HttpResponse("Invalid Request Method", status=405)





@login_required
def InsertOKDomain(request):
    blacklist = ['powerapps']
    if request.method == 'POST':
        form = OKDomainsForm(request.POST)
        if form.is_valid():
            # Extract form data and create a new OKDomains object
            domain = form.cleaned_data['domain']

            # Check if domain is in blacklist 
            for entry in blacklist:
                if entry in str(domain):
                    context = {'uploadmessage': 'Error: Domain is blacklisted'}
                    return render(request, 'domains_upload.html', context)

            okdomains, created = OKDomains.objects.get_or_create(domain=domain)

            if created:
                # New domain, set all fields from form data
                okdomains.registrar = form.cleaned_data['registrar']
                okdomains.server = form.cleaned_data['server']
                okdomains.system = form.cleaned_data['system']
                okdomains.system_owner = form.cleaned_data['system_owner']
                okdomains.comments = form.cleaned_data['comments']
                okdomains.changes_since_last = "Initial upload"
            else:
                # Existing domain, update fields only if not empty
                if form.cleaned_data['registrar']:
                    okdomains.registrar = form.cleaned_data['registrar']
                if form.cleaned_data['server']:
                    okdomains.server = form.cleaned_data['server']
                if form.cleaned_data['system']:
                    okdomains.system = form.cleaned_data['system']
                if form.cleaned_data['system_owner']:
                    okdomains.system_owner = form.cleaned_data['system_owner']
                if form.cleaned_data['comments']:
                    okdomains.comments = form.cleaned_data['comments']
                okdomains.changes_since_last = "Updated from upload"

            okdomains.save()
            context = {'uploadmessage': 'Upload completed'}
            return render(request, 'domains_upload.html', context)

        elif request.method == 'POST' and request.FILES:
            # Read CSV file and update database
            try:
                file = request.FILES['file']
                decoded_file = file.read().decode('ISO-8859-1').splitlines()
                reader = csv.DictReader(decoded_file)

                domains = []
                for row in reader:
                    domain = row.get('domain')
                    domains.append(domain)

                # Get all existing domains from database
                existing_domains = OKDomains.objects.filter(domain__in=domains)

                # Update the existing domains
                for existing_domain in existing_domains:
                    # Update fields only if not empty
                    if row.get('registrar'):
                        existing_domain.registrar = row.get('registrar')
                    if row.get('server'):
                        existing_domain.server = row.get('server')
                    if row.get('system'):
                        existing_domain.system = row.get('system')
                    if row.get('system_owner'):
                        existing_domain.system_owner = row.get('system_owner')
                    if row.get('comments'):
                        existing_domain.comments = row.get('comments')
                    existing_domain.changes_since_last = "Updated from upload"
                    existing_domain.save()

                existing_db_domains = []
                for entry in existing_domains.values():
                    existing_db_domains.append(entry["domain"])
                reader = csv.DictReader(decoded_file)
                for row in reader:
                    domain = row.get('domain')
                    if domain not in existing_db_domains:
                        for entry in blacklist:
                            if entry not in str(domain):
                                new_domain = OKDomains(
                                    domain=domain,
                                    registrar=row.get('registrar'),
                                    server=row.get('server'),
                                    system=row.get('system'),
                                    system_owner=row.get('system_owner'),
                                    comments=row.get('comments'),
                                    changes_since_last="Added from upload"
                                )
                                new_domain.save()

                context = {'uploadmessage': 'Upload completed', 'UserData':GetUserData(request)}
                return render(request, 'domains_upload.html', context)

            except Exception as e:
                context = {'uploadmessage': 'Error during file upload: {}'.format(e), 'UserData':GetUserData(request)}
                return render(request, 'domains_upload.html', context)
    else:
        form = OKDomainsForm()
        context = {'form': form, 'UserData':GetUserData(request)}
        return render(request, 'domains_upload.html', context)

@login_required
def ActiveDirectory(request):
    return render(request, 'active_directory.html')

def LDAPInit():
    ldap_server  = 'ldaps://ldaps.oslofelles.oslo.kommune.no:636'
    username  = os.environ["CSIRT_LDAPUSER"]
    password  = os.environ["CSIRT_LDAPPASSWORD"]
    base_dn =  "DC=oslofelles,DC=oslo,DC=kommune,DC=no"
    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)
    ldap.set_option(ldap.OPT_PROTOCOL_VERSION, 3)
    ldap_conn = ldap.initialize(ldap_server)
    ldap_conn.set_option(ldap.OPT_REFERRALS, 0)
    return ldap_conn

@login_required
def ScannersPage(request):
    return render(request, 'scanners.html')


def URLScan(url):
    data = []
    base_query = 'https://urlscan.io/api/v1/search/?q=domain:{}'.format(url)
    header = {"Authorization": os.environ["URLSCAN_API_SECRET"]}
    result = requests.get(base_query, headers=header).json()
    result_total = result["total"]
    result_took = result["took"]
    for entry in result["results"]:
        data.append(entry)
    return data

@login_required
def DomainURLScan(request):
    if request.method == 'POST':
        domains = request.POST.get('domains')
        domains = domains.split(",")
        for entry in domains:
            domain = entry.replace('http://','').replace('https://','').replace('/', '').replace('www.','')
            results = URLScan(domain)
            print(results)

    return render(request, "attack_surface_domains.html")



@login_required
def TelegramDataSearchByType(request):
    if request.method == "POST":
    
        telegram_search = request.POST.get("telegram_search")
        search_type = request.POST.get("search_type")
        filters = {"telegram_search": telegram_search, "search_type": search_type}
        filters["translate"] = bool(request.POST.get("translate_checkbox"))
        filters["translate_search"] = bool(request.POST.get("translate_search_checkbox"))
        if search_type in ["channel", "message", "highlighted_words"]:
            return TelegramDataSearch(request, filters)
        else:
            print("Invalid search type.")
    else:
        context = {'UserData': GetUserData(request)}
        return render(request, 'telegram_data.html', context) 

def filter_telegram_data(telegram_data_list, filters):
    key = filters["search_type"]
    value = filters["telegram_search"]
    translate = filters["translate"]
    translate_search = filters["translate_search"]
    if translate:
        # Set the http_proxy via a shell command
        LoadProxy()
        translator = Translator()

    if translate_search:
        LoadProxy()
        translator = Translator()
        value = translator.translate(value, dest='ru').text

    working_data = []
    seen_combinations = set()

    for entry in telegram_data_list:
        # If the key is "highlighted_words" and the value is "all", select entries that have a non-empty list in highlighted_words.
        # Otherwise, select entries that match the provided value.
        pattern = r'\b' + re.escape(str(value).lower()) + r'\b'

        # If the key is "highlighted_words" and the value is "all", select entries that have a non-empty list in highlighted_words.
        # Otherwise, select entries that match the provided value.
        if key == "highlighted_words" and value == "all":
            condition = key in entry and isinstance(entry[key], str) and entry[key] != "[]"
        else:
            condition = bool(re.search(pattern, str(entry[key]).lower()))
        if condition:
            message_data = json.loads(entry["message_data"])
            try:
                entry_date = message_data["Date"].split("+")[0]
            except:
                entry_date = message_data["Date"]

            if (entry['message'], entry_date) not in seen_combinations:
                seen_combinations.add((entry['message'], entry_date))
                modified_entry = entry.copy()

                if "highlighted_words" in modified_entry:
                    highlighted = modified_entry["highlighted_words"]
                    if isinstance(highlighted, list):
                        modified_entry["highlighted_words"] = [word.encode().decode('unicode_escape') for word in highlighted]

                modified_entry["Date"] = entry_date
                if translate:
                    modified_entry["message_translated"] = translator.translate(modified_entry["message"], dest='en').text
            working_data.append(modified_entry)
    DisableProxy()
    return working_data

@login_required
def TelegramDataSearch(request, filters):
    all_telegram_data = list(TelegramData.objects.values())  # Fetching all data from the model as a list of dictionaries

    if not filters["telegram_search"] and filters["search_type"] == "highlighted_words":
        filters["telegram_search"] = "all"

    filtered_telegram_data = filter_telegram_data(all_telegram_data, filters)
    context = {'filters': filters, 'telegram_data': filtered_telegram_data, 'UserData': GetUserData(request)}
    return render(request, 'telegram_data.html', context)

def GetLatestTelegramData():
    # Enable the proxy server
    LoadProxy()

    # Fetch the last 100 entries as a buffer based on `message_date`
    highlighted_messages = list(TelegramData.objects.exclude(highlighted_words="[]").order_by('-message_date')[:100])

    # If there are no highlighted messages, return immediately
    if not highlighted_messages:
        return {'processed_list': [], 'last_entry': None}

    # Initialize the translator
    translator = Translator()

    # Track seen messages and the processed list
    seen_messages = set()
    processed_list = []

    # Fetch the last entry and its message_date
    for entry in highlighted_messages:
        # Skip the message if it's a duplicate
        if entry.message in seen_messages:
            continue

        entry.message_translated = translator.translate(entry.message, dest='en').text

        # Add the processed entry to the list and mark it as seen
        processed_list.append(entry)
        seen_messages.add(entry.message)

        # Stop once you have 3 unique messages
        if len(processed_list) == 3:
            break

    # Fetch the `date_added` of the very last entry in the database
    last_entry_in_database = TelegramData.objects.latest('date_added')
    last_entry_date_added = last_entry_in_database.date_added if last_entry_in_database else None

    DisableProxy()
    # Return processed list and the last entry's date_added
    return {'processed_list': processed_list, 'last_entry': last_entry_date_added}
