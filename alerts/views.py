from django.shortcuts import render
from django.http import HttpResponse, HttpRequest, HttpResponseRedirect
from django.shortcuts import redirect
from django.contrib.auth.decorators import login_required, user_passes_test
from django.core.files.storage import FileSystemStorage
from django.urls import reverse
import os
from django.contrib.auth.models import Group
from datetime import datetime
import scripts.get_cves as get_cves
import scripts.shodan_filter as shodan_filter
from .models import CVEScans,ShodanResults,UnlistedCVEs,QualysResults,PasswordSpray,OKDomains,QualysComments,NessusData,TelegramData
from .forms import OKDomainsForm
import scripts.send_alert_email as alert_email
from collections import defaultdict
import scripts.shodan_ as shodan
import scripts.soc_scripts as soc_scripts
from django.shortcuts import render
from django.conf import settings
from django.core.files.storage import FileSystemStorage
from django.db.models import Q
import requests
import subprocess
import json
import logging
import time
from django.db.models import F
from googletrans import Translator
import csv
import sys
import ast 
import re
try:
    import ldap
except:
    print("Could not import LDAP")


def LoadProxy():
    os.environ["http_proxy"] = "http://{}:{}@inetproxy.oslofelles.oslo.kommune.no:3128".format(os.environ["CSIRT_LDAPUSER"], os.environ["CSIRT_LDAPPASSWORD"])
    os.environ["https_proxy"] = "http://{}:{}@inetproxy.oslofelles.oslo.kommune.no:3128".format(os.environ["CSIRT_LDAPUSER"], os.environ["CSIRT_LDAPPASSWORD"])

def DisableProxy():
    os.environ.pop("http_proxy", None)
    os.environ.pop("https_proxy", None)

def is_member_of_group(group_name):
    def check(user):
        return user.is_authenticated and user.groups.filter(name=group_name).exists()
    return check

@login_required
def GetUserData(request):
    user = request.user
    user_data = {
        'username': user.username,
        'email': user.email,
        'first_name': user.first_name,
        'last_name': user.last_name,
    }
    return user_data

@login_required
def index(request):
    found_cves = get_cves.daily_cve()
    found_news = get_cves.daily_news()
    UnlistedCVEs = get_cves.unlisted_cves()
    cve_stats = GetCVEStatistics(found_cves)
    context = {'CVE_list':found_cves, 
               'CVECount':cve_stats[0], 
               'CVEStats':cve_stats[1], 
               'CVEQuery':'Siste døgn', 
               'DailyNews':found_news, 
               'UnlistedCVEs':UnlistedCVEs,
               'TelegramData':GetLatestTelegramData(), 
               'UserData':GetUserData(request)}
    return render(request,'index.html', context)

@login_required
def cve_weekly(request):
    found_cves = get_cves.weekly_cve()
    found_news = get_cves.daily_news()
    UnlistedCVEs = get_cves.unlisted_cves()
    cve_stats = GetCVEStatistics(found_cves)
    context = {'CVE_list':found_cves, 'CVECount':cve_stats[0], 'CVEStats':cve_stats[1], 'CVEQuery':'Siste 7 dager', 'DailyNews':found_news, 'UnlistedCVEs':UnlistedCVEs}
    return render(request,'index.html', context)

@login_required
def cve_monthly(request):
    found_news = get_cves.daily_news()
    UnlistedCVEs = get_cves.unlisted_cves()
    found_cves = get_cves.monthly_cve()
    cve_stats = GetCVEStatistics(found_cves)
    context = {'CVE_list':found_cves, 'CVECount':cve_stats[0], 'CVEStats':cve_stats[1], 'CVEQuery':'Denne måneden', 'DailyNews':found_news, 'UnlistedCVEs':UnlistedCVEs}
    return render(request,'index.html', context)

@login_required
def cve_all(request):
    found_news = get_cves.daily_news()
    UnlistedCVEs = get_cves.unlisted_cves()
    found_cves = get_cves.all_cve()
    cve_stats = GetCVEStatistics(found_cves)
    context = {'CVE_list':found_cves, 'CVECount':cve_stats[0], 'CVEStats':cve_stats[1], 'CVEQuery':'Denne måneden', 'DailyNews':found_news, 'UnlistedCVEs':UnlistedCVEs}
    return render(request,'index.html', context)


def GetCVEStatistics(cve_list):
    severity = {'Kritisk':0,'Høy':0,'Medium':0,'Lav':0,'N/A':0}
    cve_count = len(cve_list)
    for cve in cve_list:
        if cve["cvss_score"] == "N/A":
            severity["N/A"] +=1
        elif cve["cvss_score"] >= 9:
            severity["Kritisk"] += 1
        elif cve["cvss_score"] >= 7.5:
            severity["Høy"] += 1
        elif cve["cvss_score"] >= 5:
            severity["Medium"] += 1
        else:
            severity["Lav"] += 1
    return cve_count, severity

@login_required
def send_alert(request):
    if request.method == "POST":
        selected_cves = request.POST.getlist("send_alert")
        if len(selected_cves) == 0:
            context = {'AlertMessage':'Du må velge CVEer fra CVE listen for å kunne sende en varsling.'}
        else:
            context = {'Selected_CVEs':get_cves.single_cve(selected_cves)}
    return render(request,'send_alert.html', context)

@login_required
def send_alert_email(request):
    if request.method == "POST":
        table = request.POST.get("CVE_Table")
        receiver = request.POST.get("alert_receiver")
        cc = request.POST.get("alert_cc")
        alert_text = request.POST.get("alert_text")
        CVE = request.POST.getlist("CVEs")
        context = {'receiver':receiver, 'cc':cc, 'description':alert_text, 'CVES':CVE}
        alert_email.main(context)
        found_cves = get_cves.daily_cve()
        scan_info = str(*CVEScans.objects.filter(scan_type="daily").values('scan_end').last().values())
        context = {'message':'<b>Sucess</b>: E-mail sent to: {}'.format(receiver),'CVE_list':found_cves, 'CVE_Count':len(found_cves), 'Query':'Realtime CVE', 'Last_scan':scan_info}
        return render(request,'index.html', context)

@login_required
def GoogleNewsScraper(request):
    context = {'UnlistedCVEs':UnlistedCVEs.objects.values(), 'UserData':GetUserData(request)}
    return render(request,'new_cve_scraper.html', context)

@login_required
def Shodan(request):
    shodan_info = shodan.GetResults(0)
    context = {'ShodanResults':shodan_info[0], 'Shodan_Stats':shodan_info[1], 'ShodanScans':shodan_info[2], 'Vulns':shodan_info[3], 'UserData':GetUserData(request)}
    return render(request,'shodan.html', context)

@login_required
def update_shodan_entry(request):
    if request.method == "POST":
        comments = request.POST.getlist("comment")
        ids = request.POST.getlist("entry_id")
        for x,y in zip(ids, comments):
            entry = ShodanResults.objects.get(id=x)
            entry.entry_comments = y
            entry.save(update_fields=['entry_comments'])
    shodan_info = shodan.GetResults(0)
    context = {'ShodanResults':shodan_info[0], 'Shodan_Stats':shodan_info[1], 'ShodanScans':shodan_info[2], 'Vulns':shodan_info[3], 'UserData':GetUserData(request)}
    return render(request,'shodan.html', context)


@login_required
def ShodanSearch(request, filters=None):
    if request.method == "POST":
        if not filters:
            filters = ShodanGetFilters(request) 
        shodan_info = shodan.GetResults(0)
        context = {'ShodanResults':ShodanDataFilter(ShodanSearchQuery(filters)[0], filters), 'filters':filters, 'UserData':GetUserData(request)}
        return render(request,'shodan.html', context)
    else:
        shodan_info = shodan.GetResults(0)
        context = {'ShodanResults':shodan_info[0], 'Shodan_Stats':shodan_info[1], 'ShodanScans':shodan_info[2], 'Vulns':shodan_info[3], 'filters':filters, 'UserData':GetUserData(request)}
        return render(request,'shodan.html', context)

def ShodanGetFilters(request):
    filters = {
        "webservers": request.POST.get("webserver_checkbox"), 
        "ssh":request.POST.get("ssh_checkbox"),
        "others":request.POST.get("others_checkbox"),
        }
    return filters

def ShodanSearchQuery(filters):
    # Construct the base query set
    base_query = ShodanResults.objects.all()

    # Get the query results
    results = base_query.values()
    shodan_server_results = shodan.GetResults(0, results)
    return shodan_server_results

def ShodanDataFilter(shodan_data, filters):
    webserver_shodan_data = []
    ssh_shodan_data = []
    others_shodan_data = []

    for entry in shodan_data:
        if "ssh" in entry["entry_data"]["product"].lower() or entry["entry_data"]["port"] == 22:
            ssh_shodan_data.append(entry)
        else:
            target_words = ["apache", "http", "iis", "jetty", "nginx"]
            product_data = entry["entry_data"]["product"]
            entry["has_target_word"] = any(word in product_data.lower() for word in target_words)
            status_code = entry["entry_data"]["http"]["status"] if "http" in entry["entry_data"] else None
            if entry["has_target_word"] or entry["entry_data"]["port"] in [80, 443] or (status_code and 200 <= status_code <= 399):
                if entry not in webserver_shodan_data:
                    webserver_shodan_data.append(entry)
            else:
                others_shodan_data.append(entry)

    if filters["ssh"]:
        if filters["webservers"]:
            return ssh_shodan_data.extend(webserver_shodan_data)
        else:
            return ssh_shodan_data
    else:
        if filters["webservers"]:
            return webserver_shodan_data
        else:
            return others_shodan_data



@login_required
def SOC(request):
    context = {'':''}
    return render(request,'soc.html', context)

@login_required
def SOC_Scan_Url(request):
    if request.method == "POST":
        url = request.POST.get("scan_url_entry")
        context = {'URLScan':soc_scripts.URLScanIO(url)}
        return render(request,'soc.html', context)

def GetQualysStats(qualys_results, current_query):
    severity = {'5':0,'4':0,'3':0,'2':0,'1':0}
    internet_exposed = 0
    internet_exposed_ips = []
    for entry in qualys_results:
        severity["{}".format(entry["severity"])] += 1
        if entry["internet_exposed"] == "1":
            internet_exposed += 1
            if entry["hostname"] not in internet_exposed_ips:
                internet_exposed_ips.append(entry["hostname"])
    qualys_stats = {'Timestamp':GetQualysLastEntry(),'Current query':current_query, 'Amount of entries':len(qualys_results), 'Severity':severity, 'Entries with internet exposed':internet_exposed,'Unique internet exposed assets': len(internet_exposed_ips)}
    return qualys_stats


def GetQualysLastEntry():
    # Fetches the latest entry from thr qualys database.
    # This is to ensure that the newest data is shown.
    last_entry = QualysResults.objects.values().last()["scan_time"]
    return last_entry

def GetCisaVulns():
    # Fetches the CISA known exploited vulnerabilities from the CISA website.
    cve_list = []
    url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
    try:
        LoadProxy()
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if one occurred.
        cisa_data = response.json()
        
        for entry in cisa_data["vulnerabilities"]:
            if entry["cveID"] not in cve_list:
                cve_list.append(entry["cveID"])
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from {url}: {e}")
        return []
    finally:
        DisableProxy()
    return cve_list

def UpdateQualysCISAField(entry_id, entry_value):
    # Updates the known_exploited field in the QualysDB
    entry = QualysResults.objects.get(id=entry_id)
    entry.known_exploited = entry_value
    entry.save()

def GetQualysBS():
    business_services = ['All data'] + list(QualysResults.objects.exclude(bs__isnull=True).exclude(bs__exact='').values_list('bs', flat=True).distinct())
    sorted_business_services = sorted(business_services)
    return sorted_business_services

@login_required
def Qualys(request):
    last_entry = GetQualysLastEntry()
    context = {'filters':{'dataset':GroupChecker(request), 'last_entry':last_entry}, 'UserData':GetUserData(request), 'business_service':GetQualysBS()}
    return render(request,'qualys.html', context)

def KartoteketAPIBSCheck(request):

    # Checks the Kartoteket API to see if the user is manager of a business service.
    # This is to ensure that the user can automatically view the data assigned to them.
    email = request.user.email
    url = "https://kartoteket.oslo.kommune.no//get-api/tilganger/?email={}".format(email)
    header = {"key": os.environ["KARTOTEKET_NETWORK_API_SECRET_BS"]}
    result = requests.get(url, headers=header).json()["business_services"]
    return result

def ManualCheck(request):
    groups = {
        "CSIRT": ["all"],
        "ITAS": ["OK-ITAS"],
        "ACOS_UKE": ["OK-Acos - Standard tjeneste"]
    }
    user_groups = []
    for group in groups:
        if is_member_of_group(group)(request.user):
            user_groups.extend(groups[group])
    return user_groups

def GroupChecker(request):
    try:
        dataset = KartoteketAPIBSCheck(request)
        if dataset:
            return dataset
        else:
            dataset = ManualCheck(request)
    except Exception as e:
        dataset = ManualCheck(request)
    return dataset


def QualysSearchQuery(filters):
    # Create a dictionary to map filter types to fields
    filter_mapping = {
        "hostname": "hostname",
        "bss": "bss",
        "bs": "bs",
        "system": "system",
        "cve": "cve",
        "vulnerability_name": "title",
        "filepath": "filepath",
        "internal_ip": "ip",
        "external_ip": "internet_exposed",
        "comments": "comments"
    }
    
    # Construct the base query set
    base_query = QualysResults.objects.filter(scan_time=GetQualysLastEntry())
    
    # Apply filters
    if filters["dataset"] != ["all"]:
        base_query = base_query.filter(bs__in=filters["dataset"])
    if filters["internet_exposed"]:
        base_query = base_query.exclude(internet_exposed__contains="e")
    if filters["critical_vulns"]:
        base_query = base_query.filter(severity="5")
    if filters["known_exploited"]:
        base_query = base_query.filter(known_exploited="1")
    
    # Add the search field filter
    search_field = filter_mapping[filters["search_type"]]
    search_value = filters["vuln_search"]
    search_query = {f"{search_field}__contains": search_value}
    base_query = base_query.filter(**search_query)
    
    # Remove entries containing "Update for kernel" if filter_kernel is true
    if filters["filter_kernel"]:
        base_query = base_query.exclude(title__contains="Update for kernel")
    if filters["filter_false"]:
        base_query = base_query.exclude(false_positive=1)
    
    # Check and apply selected_service filter
    selected_service = filters.get("selected_service")
    if selected_service and selected_service != "All data":
        base_query = base_query.filter(bs__contains=selected_service)
    
    # Get the query results
    qualys_server_results = base_query.values()
    return QualysDataFilter(qualys_server_results)

def QualysOverview(qualys_data, filters):
    grouped_data = {}
    total_count = 0

    for entry in qualys_data:
        title = entry['title']
        if title in grouped_data:
            grouped_data[title]['count'] += 1
        else:
            grouped_data[title] = {'title': title, 'count': 1}

        if 'severity' not in grouped_data[title]:
            grouped_data[title]['severity'] = entry['severity']

        if 'cve' not in grouped_data[title]:
            grouped_data[title]['cve'] = entry['cve']

        if 'hostnames' not in grouped_data[title]:
            grouped_data[title]['hostnames'] = []
        grouped_data[title]['hostnames'].append(entry['hostname'])

        # Fetch comments from QualysComments table based on title

        comments = QualysComments.objects.filter(title=title, filters=MapQualysCommentsFilters(filters))
        comment_list = [comment.comment for comment in comments]
        grouped_data[title]['comments'] = ", ".join(comment_list)

        total_count += 1  # Increment the total count

    result = sorted(grouped_data.values(), key=lambda x: x['count'], reverse=True)
    return {'data': result, 'total_count': total_count}


def update_comments_for_qualys_results(entries):
    for entry in entries:
        title = entry.get("title")
        comment = entry.get("comment")
        if comment and len(comment) > 5:
            entries_to_update = QualysResults.objects.filter(title=title)
            for item in entries_to_update:
                if item.comments != comment:
                    item.comments = comment
                    item.save()

def update_comments_for_qualys_comments(filters, servers, entries):
    mapped_filters = MapQualysCommentsFilters(filters)
    for entry in entries:
        title = entry.get("title")
        comment = entry.get("comment")
        if comment and len(comment) > 5:
            obj, created = QualysComments.objects.get_or_create(
                title=title, filters=mapped_filters, servers=str(servers))
            if obj.comment != comment:
                obj.comment = comment
                obj.save()

def UpdateQualysComments(request):
    if request.method == "POST":

        # Extract filters from the request
        filters = dict(eval(request.POST.get("filters")))
        # Filter out the keys based on their prefixes
        comment_keys = [k for k in request.POST.keys() if k.startswith("comment_")]
        team_keys = [k for k in request.POST.keys() if k.startswith("team_")]
        false_positive_keys = [k for k in request.POST.keys() if k.startswith("false_positive_checkbox-")]

        # Set all false_positives to 0
        all_entry_ids = [int(key.split("_")[1]) for key in comment_keys]
        QualysResults.objects.filter(id__in=all_entry_ids).update(false_positive=0)

        # Fetch values based on the filtered keys
        comments = [request.POST[k] for k in comment_keys]
        teams = [request.POST[k] for k in team_keys]
        false_positives = [request.POST[k] for k in false_positive_keys]

        for key, comment in zip(comment_keys, comments):
            entry_id = int(key.split("_")[1])  # Assuming the key is in the format "comment_ID"
            
            # Only update comment if it's not None or not an empty string
            if comment and comment.strip():
                try:
                    QualysResults.objects.filter(id=entry_id).update(comments=comment)
                except Exception as e:
                    # Handle exceptions (like not finding the entry) here if needed
                    print(f"Error updating comment for entry ID {entry_id}: {e}")

        for key, team in zip(team_keys, teams):
            entry_id = int(key.split("_")[1])  # Assuming the key is in the format "team_ID"
            
            # Only update team if it's not None or not an empty string
            if team and team.strip():
                try:
                    QualysResults.objects.filter(id=entry_id).update(team=team)
                except Exception as e:
                    # Handle exceptions here if needed
                    print(f"Error updating team for entry ID {entry_id}: {e}")

        for key, false_positive in zip(false_positive_keys, false_positives):
            try:
                entry_id = int(key.split("-")[1])  # Attempt to extract the ID
            except ValueError:
                # Couldn't convert to int, so skip this key
                continue
    
            false_positive_value = 1 if false_positive == "1" else 0  # Convert the checkbox value to 1 or 0

            try:
                QualysResults.objects.filter(id=entry_id).update(false_positive=false_positive_value)
            except Exception as e:
                # Handle exceptions here if needed
                print(f"Error updating false_positive for entry ID {entry_id}: {e}")


        # Call the QualysSearch function with the extracted filters
        return QualysSearch(request, filters)





def MapQualysCommentsFilters(filters):
    new_filters = {
    'dataset':filters["selected_service"], 
    'internet_exposed':filters["internet_exposed"],
    'critical_vulns':filters["critical_vulns"], 
    'known_exploited':filters["known_exploited"]
    }
    return new_filters


def QualysDataFilter(qualys_data):
    checklist = []
    last_detected = GetQualysLastEntry()  # Assuming GetQualysLastEntry() returns the desired value 
    for entry in qualys_data:
        if entry["id"] not in checklist:
            checklist.append([entry["id"], entry]) 
    qualys_data_filtered = []
    for entry in checklist:
        if entry[1]["systemmanager"]:
            entry[1]["systemmanager"] = entry[1]["systemmanager"].translate(str.maketrans('', '', "[]'"))
        if "e" in entry[1]["internet_exposed"]:
            entry[1]["internet_exposed"] = "N/A"      
        if entry[1]["last_fixed"] == "NaT":
            entry[1]["last_fixed"] = "N/A"        
        if entry[1]["cve"] == "nan":
            entry[1]["cve"] = "N/A"
        if entry[1]["last_detected"].split()[0] != last_detected:
            continue  # Skip this entry if last_detected is not equal to the desired value
        qualys_data_filtered.append(entry[1])
    return qualys_data_filtered

def QualysGetStats(qualys_results):
    pass

def QualysGetFilters(request, dataset):
    last_entry = GetQualysLastEntry()
    filters = {
        "last_entry":last_entry,
        "dataset": dataset,
        "filepath": request.POST.get("filepath_checkbox"), 
        "internet_exposed":request.POST.get("internetex_checkbox"),
        "overview":request.POST.get("overview_checkbox"),
        "critical_vulns": request.POST.get("critical_vulns_checkbox"), 
        "all_entries":request.POST.get("all_checkbox"), 
        "known_exploited":request.POST.get("knownex_checkbox"),
        "vuln_search":request.POST.get("vuln_search"),
        "search_type":request.POST.get("search_type"),
        "selected_service":request.POST.get("selected_service"),
        "systeminfo":request.POST.get("systeminfo_checkbox"),
        "business_service":request.POST.get("business_service_checkbox"),
        "comments":request.POST.get("comments_checkbox"),
        "filter_kernel":request.POST.get("filter_kernel_checkbox"),
        "filter_false":request.POST.get("filter_false_checkbox"),
        }
    return filters

@login_required
def QualysSearch(request, filters=None):
    if request.method == "POST":
        business_services = GetQualysBS()
        if not filters:
            filters = QualysGetFilters(request, GroupChecker(request))
        qualys_results = QualysSearchQuery(filters)        
        if filters["overview"]:
            context = {'QualysResults':qualys_results, "filters":filters, "overview_table":QualysOverview(qualys_results, filters), 'UserData':GetUserData(request), 'business_service': business_services}
        else:
            context = {'QualysResults':qualys_results, "filters":filters, 'UserData':GetUserData(request), 'business_service':business_services}
        return render(request,'qualys.html', context)
    else:
        message = "Du må velge et filter for å vise resultater..."
        context = {'message':message, 'UserData':GetUserData(request)}
        return render(request,'qualys.html', context)

@login_required
def ExposedPasswords(request):
    data = PasswordSpray.objects.values()
    for entry in data:
        for item in list(eval(entry["userinfo"])):
            if entry["user"] in str(item):
                for x in item:
                    if type(x) == dict:
                        entry["groups"] = x["memberOf"]
    context = {'data':data, 'UserData':GetUserData(request)}
    return render(request,'exposed_passwords.html', context)

@login_required
def VulnerabilityUpload(request):
    if request.method == 'POST':
        file = request.FILES['file']
        filename = file.name
        upload_path = '/var/csirt/source/CVE-WEB/uploads/qualys_vulns/'

        # Get the selected vulnerability type from the dropdown menu
        vulnerability_type = request.POST.get('vulnerability_type')

        # Check if the selected vulnerability type is Qualys or Defender and set the filename accordingly
        if vulnerability_type == 'Qualys':
            new_filename = 'QualysVulnerabilities.xlsx'
            script_path = '/var/csirt/source/CVE-WEB/scripts/updatequalys.sh'
        elif vulnerability_type == 'Defender':
            new_filename = 'DefenderVulnerabilities.xlsx'
            script_path = '/var/csirt/source/CVE-WEB/scripts/updatedefender.sh'
        else:
            # Return an error message if the selected vulnerability type is invalid
            context = {'filename': '', 'upload_result': 'error', 'output': '', 'error': 'Invalid request'}
            return render(request, 'qualys_upload.html', context)

        file_path = os.path.join(upload_path, new_filename)

        # Remove the existing "Vulnerabilities.xlsx" file if it exists
        existing_file_path = os.path.join(upload_path, new_filename)
        if os.path.isfile(existing_file_path):
            os.remove(existing_file_path)

        # Save the uploaded file with the appropriate filename
        with open(file_path, 'wb') as f:
            for chunk in file.chunks():
                f.write(chunk)

        # Run the appropriate script based on the selected vulnerability type
        process = subprocess.Popen(['/bin/bash', script_path, file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, error = process.communicate()

        # Decode the output and error from bytes to string
        output = output.decode()
        error = error.decode()

        # Render the qualys_upload.html template with the upload result and script output
        context = {'filename': new_filename, 'upload_result': 'success', 'output': output, 'error': error, 'UserData':GetUserData(request)}
        return render(request, 'qualys_upload.html', context)

    return render(request, 'qualys_upload.html')



def GetLatestNessusData():
    latest_nessus_data = NessusData.objects.order_by('-date').first()
    if latest_nessus_data:
        latest_data = json.loads(latest_nessus_data.data)
        dataset = latest_data.get("data")
        return dataset

@login_required
def AttackSurface(request):
    vulnerabilities = process_vulnerabilities(GetLatestNessusData())
    context = {'Vulnerabilities': vulnerabilities, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface.html', context)

def AttackSurfaceVulnStatistics(vulnerabilities):
    statistics = {
        'total_vulnerabilities': len(vulnerabilities),
        'total_CVE': 0,
        'average_CVSS': 0,
        'risk_count': {
            'Critical': 0, 
            'High': 0,
            'Medium': 0,
            'Low': 0,
            'None': 0,
            'Other': 0
        },

    }

    total_cvss_score = 0

    for vuln in vulnerabilities:
        # Counting CVEs
        if vuln.get("CVE"):
            statistics['total_CVE'] += 1
       

        print(vuln["vuln"])


    return statistics





@login_required
def AttackSurfaceSearchDomain(request, domain):
    try:
        domain_entry = OKDomains.objects.filter(domain=domain).values()[0]
        vulnerabilities = process_vulnerabilities(json.loads(domain_entry["vulnerabilities"]))
        filters = {'search_type':'domene', 'vuln_search':domain}
        context = {'SearchType':'domene','okdomains': domain_entry, 'Domain': domain, 'Vulnerabilities': vulnerabilities, 'UserData': GetUserData(request), 'filters':filters, 'statistics':AttackSurfaceVulnStatistics(vulnerabilities)}
        return render(request, 'attack_surface_domain_search.html', context)
    except:
        context = {'SearchType':'domene', 'Domain': domain, 'UserData': GetUserData(request)}
        return render(request, 'attack_surface_domain_search.html', context)




@login_required
def AttackSurfaceSearchByType(request):
    if request.method == "POST":
        vuln_search = request.POST.get("vuln_search")
        search_type = request.POST.get("search_type")
        filters = {"vuln_search":vuln_search, "search_type":search_type}

        if search_type == "domain":
            # Construct the URL using the provided domain value
            redirect_url = reverse('attack_surface_search_by_domain', kwargs={'domain': vuln_search})
            return HttpResponseRedirect(redirect_url)

        elif search_type in ["cve", "description", "name", "synopsis"]:
            return AttackSurfaceSearch(request, filters)

        else:
            print("Invalid search type.")
            # You might want to return a response here to handle the invalid search type scenario

    else:
        return render(request, 'attack_surface_search_form.html', {'UserData': GetUserData(request)})


def filter_vulnerabilities(vulnerabilities, key, value):
    filtered_vulnerabilities = []
    for entry in vulnerabilities:
        if str(value).lower() in str(entry["vuln"][key]).lower():
            filtered_vulnerabilities.append(entry)
    return filtered_vulnerabilities

@login_required
def AttackSurfaceSearch(request, filters):
    dataset = GetLatestNessusData()
    vulnerabilities = process_vulnerabilities(dataset)
    
    filtered_vulnerabilities = []

    if filters["search_type"] == "cve":
        filtered_vulnerabilities = [entry for entry in vulnerabilities if str(entry["vuln"]["CVE"]).lower() == str(filters["vuln_search"]).lower()]
    elif filters["search_type"] == "description":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Description", filters["vuln_search"])
    elif filters["search_type"] == "name":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Name", filters["vuln_search"])
    elif filters["search_type"] == "synopsis":
        filtered_vulnerabilities = filter_vulnerabilities(vulnerabilities, "Synopsis", filters["vuln_search"])

    context = {'filters':filters, 'Vulnerabilities': filtered_vulnerabilities, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface.html', context)



def process_vulnerabilities(dataset):
    grouped_entries = defaultdict(lambda: {'vuln': {}, 'affected_hosts': set()})

    def IterateEntry(entry):
        entry_name = entry.get("Name")
        if entry_name:
            vulnerability_info = {
                "Plugin_ID": entry.get("Plugin ID"),
                "CVE": entry.get("CVE") if entry.get("CVE") else None,
                "CVSS": float(entry.get("CVSS v2.0 Base Score")) if entry.get("CVSS v2.0 Base Score") else float(0.0),
                "Name": entry_name,
                "Risk": entry.get("Risk"),
                "Synopsis": entry.get("Synopsis"),
                "Description": entry.get("Description"),
                "Solution": entry.get("Solution"),
                "See_Also": entry.get("See Also"),
                "Plugin_Output": entry.get("Plugin Output")
            }

            affected_host = {
                "Host": entry.get("Host"),
                "Protocol": entry.get("Protocol"),
                "Port": entry.get("Port")
            }

            grouped_entries[entry_name]['vuln'] = vulnerability_info
            grouped_entries[entry_name]['affected_hosts'].add(affected_host["Host"])  # Using a set to remove duplicates

    if isinstance(dataset, list):
        for entry in dataset:
            IterateEntry(entry)
    else:
        IterateEntry(dataset)

    # Convert the grouped entries dictionary into a list of dictionaries
    grouped_lists = list(grouped_entries.values())
    
    # Sort the grouped lists based on the highest CVSS value within 'vuln'
    grouped_lists.sort(key=lambda x: ('Critical' not in x['vuln']['Risk'], -x['vuln']['CVSS']))
    return grouped_lists


@login_required
def AttackSurfaceSearchPlugin(request, plugin_id):
    if request.method == "GET":
        query = Q(vulnerabilities__contains=f'"Plugin ID": "{plugin_id}"')
        matching_entries = OKDomains.objects.filter(query).values()

        vuln_data = None
        for item in matching_entries:
            vulnerabilities = json.loads(item["vulnerabilities"])
            for vuln in vulnerabilities:
                if str(vuln.get("Plugin ID")) == str(plugin_id):
                    vuln_data = process_vulnerabilities([vuln])[0]['vuln']
                    break

        context = {'Vulnerabilities': matching_entries, 'Plugin_ID': plugin_id, 'vuln_data': vuln_data, 'UserData': GetUserData(request)}
        return render(request, 'attack_surface_vuln_search.html', context)
    

@login_required
def AttackSurfaceDomains(request):
    okdomains = OKDomains.objects.all()
    
    # Parse the JSON data in the vulnerabilities field for each domain
    for domain in okdomains:
        if domain.vulnerabilities:
            domain.vulnerabilities = list(eval(domain.vulnerabilities))
            print(domain.vulnerabilities)
        else:
            domain.vulnerabilities = []  # Set as empty list if the field is empty

    context = {'okdomains': okdomains, 'UserData': GetUserData(request)}
    return render(request, 'attack_surface_domains.html', context)

def UpdateVulDomainComments(request):    
    if request.method == "POST":
        
        plugin_id = request.POST.get("plugin_id")
        
        comment_keys = [k for k in request.POST.keys() if k.startswith("comment_")]
        
        for key in comment_keys:
            domain_name = key.replace("comment_", "")
            new_comment = request.POST[key].strip()

            if new_comment:
                try:
                    domain = OKDomains.objects.get(domain=domain_name)
                    if not domain.plugin_comments:  # Change from comments to plugin_comments
                        domain.plugin_comments = ""  # Ensure the plugin_comments attribute isn't None
                    comments_dict = dict(item.split(":") for item in domain.plugin_comments.split(",") if item)  # Change from comments to plugin_comments
                    comments_dict[plugin_id] = new_comment
                    updated_comments = ",".join(f"{k}:{v}" for k, v in comments_dict.items())
                    domain.plugin_comments = updated_comments  # Change from comments to plugin_comments
                    domain.save()
                    
                except Exception as e:
                    # This can be logged or handled appropriately
                    pass

        # Redirect back to the 'attack_surface_search_by_plugin' URL with the given plugin_id
        redirect_url = reverse('attack_surface_search_by_plugin', kwargs={'plugin_id': int(plugin_id)})
        return HttpResponseRedirect(redirect_url)
    
    else:
        return HttpResponse("Invalid Request Method", status=405)


@login_required
def InsertOKDomain(request):
    blacklist = ['powerapps']
    if request.method == 'POST':
        form = OKDomainsForm(request.POST)
        if form.is_valid():
            # Extract form data and create a new OKDomains object
            domain = form.cleaned_data['domain']

            # Check if domain is in blacklist 
            for entry in blacklist:
                if entry in str(domain):
                    context = {'uploadmessage': 'Error: Domain is blacklisted'}
                    return render(request, 'domains_upload.html', context)

            okdomains, created = OKDomains.objects.get_or_create(domain=domain)

            if created:
                # New domain, set all fields from form data
                okdomains.registrar = form.cleaned_data['registrar']
                okdomains.server = form.cleaned_data['server']
                okdomains.system = form.cleaned_data['system']
                okdomains.system_owner = form.cleaned_data['system_owner']
                okdomains.comments = form.cleaned_data['comments']
                okdomains.changes_since_last = "Initial upload"
            else:
                # Existing domain, update fields only if not empty
                if form.cleaned_data['registrar']:
                    okdomains.registrar = form.cleaned_data['registrar']
                if form.cleaned_data['server']:
                    okdomains.server = form.cleaned_data['server']
                if form.cleaned_data['system']:
                    okdomains.system = form.cleaned_data['system']
                if form.cleaned_data['system_owner']:
                    okdomains.system_owner = form.cleaned_data['system_owner']
                if form.cleaned_data['comments']:
                    okdomains.comments = form.cleaned_data['comments']
                okdomains.changes_since_last = "Updated from upload"

            okdomains.save()
            context = {'uploadmessage': 'Upload completed'}
            return render(request, 'domains_upload.html', context)

        elif request.method == 'POST' and request.FILES:
            # Read CSV file and update database
            try:
                file = request.FILES['file']
                decoded_file = file.read().decode('ISO-8859-1').splitlines()
                reader = csv.DictReader(decoded_file)

                domains = []
                for row in reader:
                    domain = row.get('domain')
                    domains.append(domain)

                # Get all existing domains from database
                existing_domains = OKDomains.objects.filter(domain__in=domains)

                # Update the existing domains
                for existing_domain in existing_domains:
                    # Update fields only if not empty
                    if row.get('registrar'):
                        existing_domain.registrar = row.get('registrar')
                    if row.get('server'):
                        existing_domain.server = row.get('server')
                    if row.get('system'):
                        existing_domain.system = row.get('system')
                    if row.get('system_owner'):
                        existing_domain.system_owner = row.get('system_owner')
                    if row.get('comments'):
                        existing_domain.comments = row.get('comments')
                    existing_domain.changes_since_last = "Updated from upload"
                    existing_domain.save()

                existing_db_domains = []
                for entry in existing_domains.values():
                    existing_db_domains.append(entry["domain"])
                reader = csv.DictReader(decoded_file)
                for row in reader:
                    domain = row.get('domain')
                    if domain not in existing_db_domains:
                        for entry in blacklist:
                            if entry not in str(domain):
                                new_domain = OKDomains(
                                    domain=domain,
                                    registrar=row.get('registrar'),
                                    server=row.get('server'),
                                    system=row.get('system'),
                                    system_owner=row.get('system_owner'),
                                    comments=row.get('comments'),
                                    changes_since_last="Added from upload"
                                )
                                new_domain.save()

                context = {'uploadmessage': 'Upload completed', 'UserData':GetUserData(request)}
                return render(request, 'domains_upload.html', context)

            except Exception as e:
                context = {'uploadmessage': 'Error during file upload: {}'.format(e), 'UserData':GetUserData(request)}
                return render(request, 'domains_upload.html', context)
    else:
        form = OKDomainsForm()
        context = {'form': form, 'UserData':GetUserData(request)}
        return render(request, 'domains_upload.html', context)

@login_required
def ActiveDirectory(request):
    return render(request, 'active_directory.html')

def LDAPInit():
    ldap_server  = 'ldaps://ldaps.oslofelles.oslo.kommune.no:636'
    username  = os.environ["CSIRT_LDAPUSER"]
    password  = os.environ["CSIRT_LDAPPASSWORD"]
    base_dn =  "DC=oslofelles,DC=oslo,DC=kommune,DC=no"
    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_NEVER)
    ldap.set_option(ldap.OPT_PROTOCOL_VERSION, 3)
    ldap_conn = ldap.initialize(ldap_server)
    ldap_conn.set_option(ldap.OPT_REFERRALS, 0)
    return ldap_conn

@login_required
def ScannersPage(request):
    return render(request, 'scanners.html')


def URLScan(url):
    data = []
    base_query = 'https://urlscan.io/api/v1/search/?q=domain:{}'.format(url)
    header = {"Authorization": os.environ["URLSCAN_API_SECRET"]}
    result = requests.get(base_query, headers=header).json()
    result_total = result["total"]
    result_took = result["took"]
    for entry in result["results"]:
        data.append(entry)
    return data

@login_required
def DomainURLScan(request):
    if request.method == 'POST':
        domains = request.POST.get('domains')
        domains = domains.split(",")
        for entry in domains:
            domain = entry.replace('http://','').replace('https://','').replace('/', '').replace('www.','')
            results = URLScan(domain)
            print(results)

    return render(request, "attack_surface_domains.html")



@login_required
def TelegramDataSearchByType(request):
    if request.method == "POST":
    
        telegram_search = request.POST.get("telegram_search")
        search_type = request.POST.get("search_type")
        filters = {"telegram_search": telegram_search, "search_type": search_type}
        filters["translate"] = bool(request.POST.get("translate_checkbox"))
        filters["translate_search"] = bool(request.POST.get("translate_search_checkbox"))
        if search_type in ["channel", "message", "highlighted_words"]:
            return TelegramDataSearch(request, filters)
        else:
            print("Invalid search type.")
    else:
        context = {'UserData': GetUserData(request)}
        return render(request, 'telegram_data.html', context) 

def filter_telegram_data(telegram_data_list, filters):
    key = filters["search_type"]
    value = filters["telegram_search"]
    translate = filters["translate"]
    translate_search = filters["translate_search"]
    if translate:
        # Set the http_proxy via a shell command
        LoadProxy()
        translator = Translator()

    if translate_search:
        LoadProxy()
        translator = Translator()
        value = translator.translate(value, dest='ru').text

    working_data = []
    seen_combinations = set()

    for entry in telegram_data_list:
        # If the key is "highlighted_words" and the value is "all", select entries that have a non-empty list in highlighted_words.
        # Otherwise, select entries that match the provided value.
        pattern = r'\b' + re.escape(str(value).lower()) + r'\b'

        # If the key is "highlighted_words" and the value is "all", select entries that have a non-empty list in highlighted_words.
        # Otherwise, select entries that match the provided value.
        if key == "highlighted_words" and value == "all":
            condition = key in entry and isinstance(entry[key], str) and entry[key] != "[]"
        else:
            condition = bool(re.search(pattern, str(entry[key]).lower()))
        if condition:
            message_data = json.loads(entry["message_data"])
            try:
                entry_date = message_data["Date"].split("+")[0]
            except:
                entry_date = message_data["Date"]

            if (entry['message'], entry_date) not in seen_combinations:
                seen_combinations.add((entry['message'], entry_date))
                modified_entry = entry.copy()

                if "highlighted_words" in modified_entry:
                    highlighted = modified_entry["highlighted_words"]
                    if isinstance(highlighted, list):
                        modified_entry["highlighted_words"] = [word.encode().decode('unicode_escape') for word in highlighted]

                modified_entry["Date"] = entry_date
                if translate:
                    modified_entry["message_translated"] = translator.translate(modified_entry["message"], dest='en').text
            working_data.append(modified_entry)
    DisableProxy()
    return working_data

@login_required
def TelegramDataSearch(request, filters):
    all_telegram_data = list(TelegramData.objects.values())  # Fetching all data from the model as a list of dictionaries

    if not filters["telegram_search"] and filters["search_type"] == "highlighted_words":
        filters["telegram_search"] = "all"

    filtered_telegram_data = filter_telegram_data(all_telegram_data, filters)
    context = {'filters': filters, 'telegram_data': filtered_telegram_data, 'UserData': GetUserData(request)}
    return render(request, 'telegram_data.html', context)

def GetLatestTelegramData():
    # Enable the proxy server
    LoadProxy()

    # Fetch the last 100 entries as a buffer based on `message_date`
    highlighted_messages = list(TelegramData.objects.exclude(highlighted_words="[]").order_by('-message_date')[:100])

    # If there are no highlighted messages, return immediately
    if not highlighted_messages:
        return {'processed_list': [], 'last_entry': None}

    # Initialize the translator
    translator = Translator()

    # Track seen messages and the processed list
    seen_messages = set()
    processed_list = []

    # Fetch the last entry and its message_date
    for entry in highlighted_messages:
        # Skip the message if it's a duplicate
        if entry.message in seen_messages:
            continue

        entry.message_translated = translator.translate(entry.message, dest='en').text

        # Add the processed entry to the list and mark it as seen
        processed_list.append(entry)
        seen_messages.add(entry.message)

        # Stop once you have 3 unique messages
        if len(processed_list) == 3:
            break

    # Fetch the `date_added` of the very last entry in the database
    last_entry_in_database = TelegramData.objects.latest('date_added')
    last_entry_date_added = last_entry_in_database.date_added if last_entry_in_database else None

    DisableProxy()
    # Return processed list and the last entry's date_added
    return {'processed_list': processed_list, 'last_entry': last_entry_date_added}
